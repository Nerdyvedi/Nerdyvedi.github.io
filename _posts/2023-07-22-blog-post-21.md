---
title: 'Managing Infrastructure using Kubernetes'
date: 2023-07-22
tags:
  - Kubernetes
  - Infrastructure
  - AWS
---

Understanding how Infrastructure is managed using Kubernetes

# Introduction to Kubernetes

*In today’s rapidly evolving world of software development, managing and scaling containerized applications has become a critical challenge. This is where Kubernetes, a powerful container orchestration technology, comes into play.*


## Introduction

Kubernetes is a “**container orchestration”** technology, released by Google back in 2015. It is somehow an open-source fork of Google's internal technology *Borg*.

## Why do we need to orchestrate containers?

### An example, better than a thousand words

Imagine that you’ve developed **myApp **application, and you have containerized it using Docker. You want to deploy it on a machine so it can be accessible to your end users.

You deploy it on **HOST1**, and you give a few users the URL from where your application is reachable, this way, end users can connect to it.

![](https://medium2.global.ssl.fastly.net/max/4160/1*byx8qbMj7AWJEJvcYWfCjQ.png)**

Then, your application starts becoming popular, and you get more users. One host will not be sufficient to absorb the load/traffic of many users, so you start deploying your application on many machines. Then you divide your users into 3 groups and you give every group a different host URL (I know, it may seem odd, but let’s imagine it for the example)

![](https://medium2.global.ssl.fastly.net/max/4232/1*VxYQc63V5ZBj__JgdcNudQ.png)**

It starts becoming **complicated to deploy** (as you need to ssh to every machine to run your container), **complicated to scale** (you don’t have an easy way to discover the hosts that have the containers), and **complicated to remain resilient** as if a container fails, you can’t see it easily in a distributed architecture…

### **Why Kubernetes?**

Kubernetes comes in to help you manage your containers. It makes sure that all the containers are running smoothly, that they have enough resources to do their work, and that they can communicate with each other properly.

Kubernetes also helps with scaling. If you need more of a particular container to handle a lot of work, Kubernetes can automatically create more of those containers to handle the load. And if a container fails, Kubernetes can start a new one to replace it, so your application keeps running...

![Simplified Kubernetes Architecture](https://medium2.global.ssl.fastly.net/max/2000/1*t-eAXo5nucjcHKJkYI5UTw.png)*Simplified Kubernetes Architecture*

Kubernetes makes use of a cluster that consists of multiple machines (often referred to as nodes) on which containers are deployed. These machines can be physical servers or virtual machines.

The core components of Kubernetes:

* API Server: Control plane component that exposes the Kubernetes API

* Scheduler: Control plane component that watches for newly created containers with no assigned node, and selects a node for them to run on.

* Kube controller manager: a component that runs controller processes.

* Kubelet: An agent that runs on each node in the cluster. It makes sure that containers are running and communicates with the control plane.

Let’s talk Kubernetes terminology, here are some terms that you will see:

![](https://medium2.global.ssl.fastly.net/max/2000/1*GFDCTi95gtp23o9x9LxCLg.png)**

* [Node](https://kubernetes.io/docs/concepts/architecture/nodes/): physical servers or virtual machines.

* [Pod](https://kubernetes.io/docs/concepts/workloads/pods/): is a group of one or more containers, with shared storage and network resources, and a specification for how to run the containers.

* [ReplicaSet](https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/): Kubernetes object that maintains a stable number of replica of identical pods running at any given time

* [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/): a declarative configuration object that specifies how to create and manage a set of identical pods in a Kubernetes cluster. It provides a way to define and control the desired state of your application’s deployment, including the number of replicas, the container image, the resource requirements, and the rollout strategy.

* [Service](https://kubernetes.io/docs/concepts/services-networking/service/): a Kubernetes object, that represents a virtual endpoint and a method for exposing a network application that is running as one or more Pods in your cluster.
> # kindly show your support by giving this message a CLAP and FOLLOW to help increase its visibility and reach a wider audience. Your engagement is greatly appreciated!

### How Kubernetes helps here?

Kubernetes has a lot of features. Let’s mention a few

**Automated scaling**

![](https://medium2.global.ssl.fastly.net/max/2000/0*ntLMXEmn_KHnAb5l.gif)**

* **Step 1**: Pod autoscaler detects that our containers are fully loaded, calculates how many additional resources we need, and requests the creation of a new Pod

* **Step 2**: We see the new **Pod **that arrives, but there’s no **Node **or machine to host it, the Pod remains in *pending* status

* **Step 3**: Cluster autoscaler detects that we need a new **Node**, so it requests a new VM to be added to the cluster

* **Step 4**: Finally, the **Pod **will be scheduled on the new **Node**

**Intelligent scheduling**

In this illustration below, we can see that the containers that we want to schedule, need some amount of CPU and RAM, and only HOST1 and HOST3 can fulfill the conditions. *(HOST2 CPU is 1, less than 1.5 requested)*

![](https://medium2.global.ssl.fastly.net/max/2000/1*mIjDAnaQ5qLpsY7MTFVbTA.png)**

When a pod is pending, and ready to be scheduled, the scheduler component of Kubernetes checks how much CPU/RAM is requested by the pod/container, to decide on which Node the pod will be scheduled.

Of course, we can do more advanced scheduling. An example would be to run some containers on Nodes that have GPUs

**Automated Self-healing**

Kubernetes architecture is based on reconciliation. In fact, you define what you want (eg: the number of replicas of your containers) and Kubernetes ensures that it will run exactly what is requested.

![](https://medium2.global.ssl.fastly.net/max/2000/1*4V-8FJUOFpjRI2RxBWUbjw.png)**

Eg: If a pod crash, Kubernetes restarts it, to be back to the desired state.

**Service discovery and load balancing**

Services serve as the primary access point for pods. Instead of relying on the specific IPs of the pods, which may change due to failures or other reasons, you can communicate with the Service itself. The Service is equipped with the knowledge of its associated pods and efficiently routes traffic to them. Whenever a new pod is created, the Service automatically discovers it.

![](https://medium2.global.ssl.fastly.net/max/2000/1*orDvErsbwkgSsHyFobhAhA.png)**

In the above example, frontend pods communicate only with “backend service”, that will load balance the traffic to the pods.

**Automated rollouts and rollbacks**

Rollouts in Kubernetes enable the deployment of new versions of an application while seamlessly redirecting traffic to it. Rollback, on the other hand, allows for reverting back to a previous version of the application. Kubernetes provides built-in support for both of them.

![](https://medium2.global.ssl.fastly.net/max/2000/0*QR_JBOxW9Xqg-b2f.gif)**

In the example above, I’m showing a rollout with a rolling upgrade strategy, meaning that you’re deploying the new application without downtime ensuring a smooth transition between 2 versions.

**Others**

Kubernetes has a LOT of features, including security, networking, and storage... we’re just scratching the surface.

<iframe src="https://medium.com/media/7b529241311369a2ea183dfb8d721923" frameborder=0></iframe>

## Conclusion

In a nutshell, Kubernetes takes care of the behind-the-scenes work of managing and coordinating containers, making it easier for people to run their applications without worrying too much about the technical details.
