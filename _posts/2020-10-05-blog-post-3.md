---
title: 'Paper Summary - A Style-Based Generator Architecture for Generative Adversarial Networks'
date: 5th October 2020
permalink: /posts/2020/10/blog-post-3/
tags:
  - cool posts
  - category1
  - category2
---
In this article we will find out what StyleGAN does, how it works, why it works.

Recap: What are GANs again ?

Let’s first step back and refresh our knowledge about Generative Adversarial Networks. The basic GAN is composed of two separate neural networks which are in continual competition against each other (adversaries).

One of these, called the generator, is tasked with the generation of new data instances that it creates from random noise, while the other, called a discriminator, evaluates these generated instances for authenticity.

Both tasks are phases in the GAN’s process cycle and are interdependent on each other. The generative phase is influenced by the discriminative phase’s evaluation, and the discriminative phase makes comparisons between the original dataset and the generated samples.

As training progresses, both networks keep getting smarter—the generator at generating fake images and the discriminator at detecting their authenticity. By the time the model has been trained, the generator manages to create an image authentic enough that the discriminator can’t tell if it’s a fake or not. Often, this final generated image is the resulting output.


Input to Progressive GAN model is a latent code(A vector in compressed space), and the model maps it to a realistic image.
For training, there is an encoder, which maps an image to the latent space, then the vector we get is fed to the generator. Training is done untill the generator
produces image similar to the one being fed to the encoder.

Now, we can do arithmitic on latent codes, and make changes to the output. 
If the latent space is perfectly disentangled , each feature would be independent of other. So, let's say we want to add a moustache to an image. How can we use stylegan to do something like this?
Encode the input image to latent space. Now, ideally all features are disentagled, so moving in a direction which adds moustaches should not add glasses(Random example).
But, How do we find this direction?
One way is to take a bunch of images with moustaches, and another bunch without one. Encode them, take the average latent code representation and subtract them.
This way we can find the difference between latent codes of mustached and non mustached people and add this difference to add a mustache.


Linear Seperability
If a latent space is disentagled, it should be able to find directions that correspond to individual factors of variations.
Generate 200k images with P(z), Use a classifier to label generated images.Remove images with low confidence. 
Got 100k labelled vectors.
Use SVM to predict label from latent code. If the model trains and produces good result, it is fairly easy to find a hyeperplane that seperates the label. If the trained SVM does not work well, the feature is entagled and we cannot find a direction.

Perceptual Path Length

Moving from a black dog to white dog of same breed. Only thing that changes is the color. 

WHAT IT DOES?
Input to the model is a latent code.
Latent means hidden. Best definition of latent space would be compressed data. 
StyleGAN generator maps a latent code to a high resolution realistic images. With some of the changes to the generator architecture, they were able to disentagle the latent space.
There is an unavoidable entaglement , as latent space follows the probability space of training data. 

Let us say, Our training dataset has all females wearing glasses. The latent space would learn this, and there would be an entanglement between these two , even though in reality there is no relation whatsoever with gender and glasses.

This entaglement is avoided embedding the latent code to an intermediate latent sapce.


WHAT IS DIFFERENT?
Traditionally(Baseline is the Progressive GAN model), latent code is provided to the generator through the first layer of the network. In this model, the input to the generator is a constant.
The latent code is first map to an intermediate latent space W, which then controls the generator through Adaptive intsance Normalization(AdaIN) at each convolution layer.

Few StyleGAN applications can be listed as follows :
1. Style Mixing
   Can be used to mix styles of 2 images. 
   ![Style mixing]('https://raw.githubusercontent.com/Nerdyvedi/Nerdyvedi.github.io/master/images/style_mixing.jpeg')
   
